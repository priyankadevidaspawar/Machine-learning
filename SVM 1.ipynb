{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf00dc7-b518-4b2c-b1f0-495193a21bb0",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM? \n",
    "A linear Support Vector Machine (SVM) aims to find a linear hyperplane that separates two classes of data points with maximum margin. The mathematical formula for a linear SVM can be represented as follows:\n",
    "\n",
    "Given a set of labeled training data {(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}, where xᵢ represents the feature vector for the i-th data point, and yᵢ represents its corresponding class label (-1 or 1), the objective of a linear SVM is to find a weight vector w and a bias term b that defines the hyperplane equation:\n",
    "\n",
    "w⋅x + b = 0\n",
    "\n",
    "where w represents the weights associated with each feature, and b represents the bias term.\n",
    "\n",
    "The decision function for a linear SVM can be written as:\n",
    "\n",
    "f(x) = sign(w⋅x + b)\n",
    "\n",
    "where sign(x) is the sign function that returns -1 if x < 0, 0 if x = 0, and 1 if x > 0.\n",
    "\n",
    "The goal of training a linear SVM is to find the optimal values for w and b that maximize the margin between the two classes, subject to the constraint that all data points are correctly classified:\n",
    "\n",
    "yᵢ(w⋅xᵢ + b) ≥ 1 for all i\n",
    "\n",
    "This constraint ensures that each data point is correctly classified and lies on the correct side of the hyperplane with a margin of at least 1. The SVM algorithm solves an optimization problem to find the values of w and b that satisfy this constraint and maximize the margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969d73f-a088-4d68-9fb0-28ce450da9a2",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM? \n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that maximizes the margin between the two classes while minimizing the classification error. This can be formulated as an optimization problem.\n",
    "\n",
    "The objective function of a linear SVM can be expressed as follows:\n",
    "\n",
    "minimize ½||w||² + CΣξᵢ\n",
    "\n",
    "subject to yᵢ(w⋅xᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0 for all i\n",
    "\n",
    "In this objective function, ||w||² represents the squared Euclidean norm of the weight vector w, which controls the margin size. The larger the norm of w, the smaller the margin, and vice versa. By minimizing ½||w||², we aim to maximize the margin.\n",
    "\n",
    "The term CΣξᵢ is the regularization parameter that balances the trade-off between maximizing the margin and allowing some misclassifications. The parameter C is a positive constant that determines the penalty for misclassification. A larger C value will result in a smaller margin but fewer misclassifications, while a smaller C value will lead to a larger margin but potentially more misclassifications.\n",
    "\n",
    "The constraints yᵢ(w⋅xᵢ + b) ≥ 1 - ξᵢ ensure that all data points are correctly classified with a margin of at least 1. The variables ξᵢ, called slack variables, allow for some misclassifications or data points that fall within the margin. The term Σξᵢ represents the sum of the slack variables.\n",
    "\n",
    "By solving this optimization problem, the SVM algorithm finds the optimal values for w and b that define the hyperplane with the maximum margin, while still satisfying the classification constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec4b81-734b-4fe1-a0b1-decd41236522",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM? \n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to efficiently handle nonlinearly separable data by implicitly mapping it to a higher-dimensional feature space. It allows SVMs to operate in a higher-dimensional space without explicitly computing the transformed feature vectors, thus avoiding the computational burden associated with high-dimensional calculations.\n",
    "\n",
    "The kernel trick works by introducing a kernel function that calculates the dot product between the feature vectors in the higher-dimensional space without explicitly computing the transformation. The kernel function takes the original input data points and maps them to a higher-dimensional space where the data becomes linearly separable. By using this kernel function, the SVM algorithm can effectively learn complex decision boundaries in the original feature space.\n",
    "\n",
    "Mathematically, given a kernel function K(xᵢ, xⱼ), where xᵢ and xⱼ represent the input feature vectors, the decision function of an SVM using the kernel trick can be written as:\n",
    "\n",
    "f(x) = sign(ΣαᵢyᵢK(xᵢ, x) + b)\n",
    "\n",
    "where αᵢ represents the Lagrange multipliers obtained during the training process, yᵢ is the class label of the training data, and b is the bias term.\n",
    "\n",
    "The kernel function K(xᵢ, xⱼ) is chosen based on the characteristics of the problem and the type of data being modeled. Commonly used kernel functions include the linear kernel, polynomial kernel, Gaussian (RBF) kernel, and sigmoid kernel, among others. Each kernel function applies a specific transformation to the original input data, allowing the SVM to find nonlinear decision boundaries in the transformed feature space.\n",
    "\n",
    "By using the kernel trick, SVMs can effectively handle complex and nonlinear data patterns without explicitly computing the transformed feature vectors, making them powerful and versatile machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddd7bb-b7d0-41e1-b8b8-b21f22387cd0",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example?\n",
    "In a Support Vector Machine (SVM), support vectors are the data points that lie closest to the decision boundary (hyperplane) and have the most influence on defining the decision boundary. These support vectors play a crucial role in determining the SVM's performance and generalization ability. Let's understand their role with an example:\n",
    "\n",
    "Suppose we have a binary classification problem with two classes, class A and class B. We want to find a decision boundary that separates these classes. The SVM aims to find the hyperplane that maximizes the margin between the classes.\n",
    "\n",
    "In this example, let's assume we have a linearly separable dataset, and the SVM successfully finds a hyperplane that separates the two classes. Some of the data points from both classes will lie directly on the hyperplane or within the margin.\n",
    "\n",
    "The support vectors are the data points from both classes that lie on the margin or are misclassified. These are the critical points that define the decision boundary because they are closest to the margin or are involved in misclassification. Support vectors have non-zero Lagrange multipliers (αᵢ) associated with them, which are obtained during the training process of the SVM.\n",
    "\n",
    "The role of support vectors can be summarized as follows:\n",
    "\n",
    "Defining the decision boundary: Support vectors determine the position and orientation of the decision boundary (hyperplane). They are the data points that influence the location of the decision boundary by being the closest to it.\n",
    "\n",
    "Margin maximization: The SVM aims to maximize the margin between the classes, which is the distance between the decision boundary and the support vectors. The support vectors lying on the margin contribute to maximizing this margin.\n",
    "\n",
    "Robustness: Support vectors represent the most critical data points. They are the ones that are most challenging to classify correctly or lie near the decision boundary. By focusing on these points, the SVM learns a robust decision boundary that generalizes well to new, unseen data.\n",
    "\n",
    "Computational efficiency: The use of support vectors allows SVMs to be computationally efficient. Instead of considering all the training data, the SVM algorithm focuses only on the support vectors, reducing the computational burden and memory requirements.\n",
    "\n",
    "In summary, support vectors are the data points that significantly contribute to defining the decision boundary and maximizing the margin in SVM. They play a vital role in shaping the SVM model and its ability to generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fec92-ec21-4f07-8dc0-34fc02e5072c",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "Certainly! Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM using examples and graphs.\n",
    "\n",
    "Hyperplane:\n",
    "In SVM, a hyperplane is a decision boundary that separates the data points of different classes. For linearly separable data, the hyperplane is a line in 2D or a plane in 3D. Here's an example with two classes (red and blue) and a linearly separable dataset:\n",
    "\n",
    "Copy code\n",
    "Red points: (+)\n",
    "Blue points: (o)\n",
    "\n",
    "       (+)\n",
    "         |\n",
    "         |\n",
    "   (o)---|---\n",
    "         |\n",
    "         |\n",
    "In this example, the hyperplane is the line that separates the red and blue points. It can be represented by the equation: w⋅x + b = 0, where w is the weight vector and b is the bias term.\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal plane in SVM refers to the planes parallel to the hyperplane that touch the support vectors. It helps define the margin, which is the distance between the marginal planes. Here's an example illustrating the marginal plane:\n",
    "\n",
    "Copy code\n",
    "Red points: (+)\n",
    "Blue points: (o)\n",
    "\n",
    "       (+)\n",
    "         |   Marginal plane\n",
    "         |      -----\n",
    "   (o)---|---   Margin\n",
    "         |      -----\n",
    "         |\n",
    "In this example, the marginal planes are represented by the dashed lines that touch the support vectors (marked as + and o). The margin is the distance between the marginal planes.\n",
    "\n",
    "Soft Margin:\n",
    "In SVM, a soft margin allows for some misclassifications or data points falling within the margin. This relaxation is useful when the data is not perfectly separable. Here's an example with a soft margin:\n",
    "\n",
    "Copy code\n",
    "Red points: (+)\n",
    "Blue points: (o)\n",
    "Misclassified: (x)\n",
    "\n",
    "    (x)    (+)\n",
    "     |     |\n",
    "     |   -----\n",
    "     |  Margin\n",
    " (o)--|-----\n",
    "     |  Margin\n",
    "    (+)\n",
    "   (x)\n",
    "In this example, the misclassified points (marked as x) are allowed to fall within the margin. The margin is still maximized, but with some tolerance for misclassifications.\n",
    "\n",
    "Hard Margin:\n",
    "In contrast to the soft margin, the hard margin SVM requires that all data points be correctly classified with a margin of at least 1. Here's an example illustrating the hard margin:\n",
    "\n",
    "Copy code\n",
    "Red points: (+)\n",
    "Blue points: (o)\n",
    " \n",
    "     (+)\n",
    "       |\n",
    "       |\n",
    "   (o)--|---\n",
    "       |\n",
    "       |\n",
    "In this example, all data points are correctly classified, and the margin is maximized while ensuring no data point lies within the margin.\n",
    "\n",
    "These illustrations demonstrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM. The choice between a soft or hard margin depends on the dataset's characteristics and the trade-off between maximizing the margin and allowing misclassifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4357967-78f7-45c4-ab33-aa64f1a17110",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of the model.\n",
    "\n",
    "ans. Sure! I'll guide you through the implementation of SVM using the Iris dataset with the specified steps. Let's get started:\n",
    "\n",
    "Step 1: Load the necessary libraries and the Iris dataset:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Consider only two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Step 2: Train a linear SVM classifier and make predictions:\n",
    "# Train a linear SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "Step 3: Compute the accuracy of the model:\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "Step 4: Plot the decision boundaries of the trained model using two features:\n",
    "# Plot the decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('SVM Decision Boundaries')\n",
    "plt.show()\n",
    "\n",
    "Step 5: Experiment with different values of the regularization parameter C:\n",
    "\n",
    "# Experiment with different values of C\n",
    "C_values = [0.1, 1, 10, 100]  # Example values of C\n",
    "\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy (C={C}):\", accuracy)\n",
    "    \n",
    "You can run the code step-by-step or altogether to see the results. The accuracy will be printed, and the decision boundaries plot will be displayed. The final step demonstrates how different values of the regularization parameter C can affect the model's performance. Feel free to modify the values or experiment with additional parameters to further explore SVM with the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64339cc7-e436-4f29-a67e-3ad2cb949f05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97e92f-6074-4fb1-bd1d-6919a32d5c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7c3b1-483c-48fe-87dd-ffedf67562d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61056857-0894-41d9-a868-5da260244456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
